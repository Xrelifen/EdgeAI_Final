import math
import torch
import torch.nn as nn
from safetensors.torch import load_model
import os

from bigtree import Node

from .sampling_utils import topk_sampling, k_sampling, heuristic_k_sampling
from ..utils import invert_mask
from ..llm import modeling_llama_no_layernorm as modeling_llama


class SSMBase(nn.Module):
    def __init__(self, config, eos_token_id=None):
        super().__init__()
        self.eos_token_id = eos_token_id
        self.config = config
    
    @classmethod
    def from_pretrained(
        cls, 
        pretrained_model_name_or_path,
        *model_args,
        config,
        torch_dtype=torch.float32,
        **model_kwargs
    ):
        draft_model_path = os.path.join(
            pretrained_model_name_or_path, "model.safetensors")
        
        model = cls(config, *model_args, **model_kwargs)
        load_model(model, draft_model_path, strict=True)
        model.to(dtype=torch_dtype)
        return model
    
    @torch.no_grad()
    def forward(self, inputs=[], **kwargs):
        raise NotImplementedError
    
    @torch.no_grad()
    def speculate(self, inputs, past_key_values, **kwargs):
        raise NotImplementedError

class SSM_EagleBase(SSMBase):
    def __init__(self, config, eos_token_id=None):
        super().__init__(config, eos_token_id)
        
        model = modeling_llama.LlamaModel(config)
        if hasattr(model, "embed_tokens"):
            del model.embed_tokens

        self.fc = nn.Linear(config.hidden_size*2, config.hidden_size, bias=True)
        self.model = model

        self.depth = 10#8
        self.topk_len = 15
        self.verify_method = "eagle"
    
    # Currently not used. This may be used to match LLM's sampling behavior.
    @torch.no_grad()
    def _sample_probs(
        self,
        logits: torch.FloatTensor,
        logits_warper,
        do_sample: bool,
        T=1.0,
    ):
        if do_sample:
            batch, seq_len, vocab_size = logits.shape
            
            logits = logits.view(-1, vocab_size)
            next_token_scores = logits_warper(None, logits)
            probs = torch.softmax(next_token_scores/T, dim=-1)
            return probs.view(batch, seq_len, vocab_size) # preserve shape
        
        else:
            return torch.softmax(logits/T, dim=-1)
    
    @torch.no_grad()
    def _update_tree_attention_data(self, depth, nodes, hidden_states, tree_mask, position_offset):
        device = hidden_states.device 
        indices = torch.tensor([node.ind for node in nodes])

        input_hidden = hidden_states[:, indices].to(device)
        
        input_ids = torch.tensor([node.id for node in nodes], device=device)[None]
        
        position_ids = torch.zeros(len(nodes), device=device)[None] + (position_offset + depth)
        
        # Generating tree masks for the new nodes, don't have to consider the old nodes
        tree_mask = tree_mask[:, :, indices]
        tree_mask = torch.concat((tree_mask, torch.eye(len(nodes), device=device, dtype=torch.bool)[None, None]), dim=3)

        return input_hidden, input_ids, position_ids, tree_mask
    
    def forward(self, inputs, embed_tokens, **kwargs):
        [hidden_states, input_ids] = inputs
        with torch.no_grad():
            inputs_embeds = embed_tokens(input_ids).to(hidden_states.dtype)
            
        hidden_states = self.fc(torch.cat((inputs_embeds, hidden_states), dim=-1))
        return self.model(inputs_embeds=hidden_states, **kwargs)
    
    @torch.no_grad()
    def _sample_nodes(sampled_probs, prev_nodes, num_sample, step):
        raise NotImplementedError
    
    @torch.no_grad()
    def speculate(self, inputs, past_key_values, embed_tokens, lm_head):
        """This method is used to draf/guess the next tokens that the LLM may generate.

        Args:
            inputs (list): A list of two tensors: hidden_states and input_ids.
            past_key_values (Cache): Cache object to store the past key-values generated by the model.
            embed_tokens (Module): embedding from LLM.
            lm_head (Module): lm_head from LLM.

        Returns:
            Node: The root node of the generated draft token tree.
        """
        [hidden_states, input_ids] = inputs
        device = hidden_states.device
        input_ids = input_ids.to(device)
        
        # take out last token as sample_token
        sample_token = input_ids[:, -1:]
        input_ids = input_ids[:, 1:]
        
        # keep original length of input_ids
        org_input_len = input_ids.shape[1] # offset of positon_id
        
        # initialize tree_mask and tree 
        tree_mask = torch.ones([1, 1, 1, org_input_len], device=device, dtype=torch.bool)
        root = Node(str(sample_token[0][0].item()), id=sample_token[0][0].item(), prob=1, global_prob=1, ind=-1)
        
        depth = 1 # depth starts from 1 in tree library
        prev_nodes = [root]
        while depth < self.depth:
            #* Decode previous nodes
            if depth == 1: # first iteration
                kv_len = past_key_values.get_seq_length()
                outputs = self(
                    inputs=[
                        hidden_states, 
                        input_ids[:, kv_len:]
                        ],
                    embed_tokens=embed_tokens,
                    past_key_values=past_key_values
                )
                hidden_states = outputs.last_hidden_state[:, -1:].clone() # Only the last token's hidden state is needed.
            else:
                hidden_states, input_ids, position_ids, tree_mask = self._update_tree_attention_data(depth, prev_nodes, hidden_states, tree_mask, org_input_len)
                outputs = self(
                    inputs=[
                        hidden_states, 
                        input_ids
                        ],
                    embed_tokens=embed_tokens, 
                    past_key_values=past_key_values,
                    position_ids=position_ids, 
                    attention_mask=invert_mask(tree_mask, dtype=hidden_states.dtype)
                )
                hidden_states = outputs.last_hidden_state

            # This is needed to properly delete outputs.logits which may be very large for first iteration
            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration
            del outputs

            #* Get the probabilities of each token
            T = 1
            sampled_probs = torch.softmax(lm_head(hidden_states)[0]/T, dim=-1)
            
            #* Sample/Select the next nodes
            next_nodes = self._sample_nodes(sampled_probs, prev_nodes, num_samples=self.topk_len, step=depth)
            
            #* depth increment
            depth += 1

            #* Append nodes to their parent nodes
            for node in next_nodes:
                prev_nodes[node.ind].append(node)
 
            #* Get the nodes as input for next iteration
            next_nodes = [node for node in next_nodes if node.id != self.eos_token_id] # don't sample nodes after eos_token_id
            prev_nodes = next_nodes
            
            #* Early stop if no nodes for next iteration
            # TODO: Also break if total_global_prob < threshold, where it does not benefit to continue
            if len(next_nodes) == 0:
                break
        
        #* Crop the tree to the max_candidate_tokens
        past_key_values.crop(org_input_len)
        
        return root

class SSM_Greedy(SSM_EagleBase):
    def __init__(self, config, eos_token_id=None):
        super().__init__(config, eos_token_id)
        self.verify_method = "greedy"

    @torch.no_grad()
    def _sample_nodes(self, sampled_probs, prev_nodes, num_samples, step):
        next_nodes = topk_sampling(sampled_probs, prev_nodes, num_samples, step)
        return next_nodes

class SSM_Stochastic(SSM_EagleBase):
    def __init__(self, config, eos_token_id=None):
        super().__init__(config, eos_token_id)
        self.verify_method = "stochastic"

    @torch.no_grad()
    def _sample_nodes(self, sampled_probs, prev_nodes, num_samples, step):
        next_nodes = k_sampling(sampled_probs, prev_nodes, num_samples, step)
        return next_nodes

class SSM_HStochastic(SSM_EagleBase):
    def __init__(self, config, eos_token_id=None):
        super().__init__(config, eos_token_id)
        self.verify_method = "hstochastic"

    @torch.no_grad()
    def _sample_nodes(self, sampled_probs, prev_nodes, num_samples, step):
        next_nodes = heuristic_k_sampling(sampled_probs, prev_nodes, num_samples, step)
        return next_nodes

# class SSM_TreeDy(SSM_EagleBase):
#     def __init__(self, config, eos_token_id=None):
#         super().__init__(config, eos_token_id)
#         self.verify_method = "treedy"

#     @torch.no_grad()
#     def _sample_nodes(self, sampled_probs, prev_nodes, num_samples, step):
#         if step <= 2:
#             next_nodes = topk_sampling(sampled_probs, prev_nodes, num_samples, step)
#         else:
#             next_nodes = k_sampling(sampled_probs, prev_nodes, num_samples, step)
            
#         return next_nodes